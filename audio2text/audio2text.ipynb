{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip uninstall -y torch torchaudio torchvision lightning pytorch-lightning"
      ],
      "metadata": {
        "id": "aUeQF8fwxUwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Y79PhPuwvgM3"
      },
      "outputs": [],
      "source": [
        "# !pip install torch==2.4.0 torchaudio==2.4.0 torchvision==0.19.0 --index-url https://download.pytorch.org/whl/cu124\n",
        "!pip install nemo_toolkit['asr']\n",
        "!apt-get install portaudio19-dev\n",
        "!pip install textgrid pyaudio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nemo.collections.asr as nemo_asr\n",
        "asr_model = nemo_asr.models.EncDecHybridRNNTCTCBPEModel.from_pretrained(model_name=\"nvidia/stt_ar_fastconformer_hybrid_large_pcd_v1.0\")\n",
        "# Define file paths\n",
        "audio_file = \"001001.wav\"\n",
        "transcription_path = \"transcript.txt\"\n",
        "audio_dir = \"audio\"\n",
        "output_dir = \"alignment_output\"\n",
        "dictionary_path = \"/content/arabic_msa.dict\"\n",
        "acoustic_model = \"arabic_msa\"\n"
      ],
      "metadata": {
        "id": "V52KSIeFvq8B",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a5f11ac-e58d-401e-f05b-5240bc662a65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2025-04-08 16:36:27 nemo_logging:393] Model EncDecHybridRNNTCTCBPEModel was successfully restored from /root/.cache/huggingface/hub/models--nvidia--stt_ar_fastconformer_hybrid_large_pcd_v1.0/snapshots/4591f979f911fe6f7cd858a434eac50ea386552c/stt_ar_fastconformer_hybrid_large_pcd_v1.0.nemo.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tasmy3"
      ],
      "metadata": {
        "id": "6UClXzzmrryC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"/content/Quran.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    quran_data = json.load(f)\n",
        "\n",
        "    quran_texts = [((s[\"surah_number\"], s[\"verse_number\"],s[\"page\"]), s[\"text\"]) for s in quran_data]\n",
        "quran_texts[:9]"
      ],
      "metadata": {
        "id": "M6qW2OP4r1pq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d11039ee-a9af-4a9c-d407-c4ee3ece4179"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[((1, 1, 1), 'بسم الله الرحمن الرحيم'),\n",
              " ((1, 2, 1), 'الحمد لله رب العالمين'),\n",
              " ((1, 3, 1), 'الرحمن الرحيم'),\n",
              " ((1, 4, 1), 'مالك يوم الدين'),\n",
              " ((1, 5, 1), 'إياك نعبد وإياك نستعين'),\n",
              " ((1, 6, 1), 'اهدنا الصراط المستقيم'),\n",
              " ((1, 7, 1), 'صراط الذين أنعمت عليهم غير المغضوب عليهم ولا الضالين'),\n",
              " ((2, 1, 2), 'الم'),\n",
              " ((2, 2, 2), 'ذلك الكتاب لا ريب فيه هدى للمتقين')]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from difflib import SequenceMatcher\n",
        "\n",
        "def find_best_match(transcription, quran_texts):\n",
        "    best_match = None\n",
        "    best_ratio = 0\n",
        "\n",
        "    for (surah_ayah_page, text) in quran_texts:\n",
        "        similarity = SequenceMatcher(None, transcription, text).ratio()\n",
        "        if similarity > best_ratio:\n",
        "            best_ratio = similarity\n",
        "            best_match = (surah_ayah_page, text, similarity)\n",
        "\n",
        "    return best_match if best_ratio > 0.6 else None\n"
      ],
      "metadata": {
        "id": "fGpUcHIKtu_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from omegaconf import open_dict\n",
        "\n",
        "decoding_cfg = asr_model.cfg.decoding\n",
        "with open_dict(decoding_cfg):\n",
        "    decoding_cfg.preserve_alignments = True\n",
        "    decoding_cfg.compute_timestamps = True\n",
        "asr_model.change_decoding_strategy(decoding_cfg)\n",
        "print(asr_model.cfg.decoding) ## If it contains preserve_alignments=True or timestamp-related fields, it likely supports alignment."
      ],
      "metadata": {
        "collapsed": true,
        "id": "qFd-bpxnMIcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.24.4"
      ],
      "metadata": {
        "id": "8GlAndyFq0iE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_file_path ='/content/001001.wav'\n",
        "transcriptions = asr_model.transcribe([audio_file_path], channel_selector=0)[0]\n",
        "\n",
        "best_match = find_best_match(transcriptions.text, quran_texts)\n",
        "\n",
        "if best_match:\n",
        "    (surah_number, ayah_number,page_number), matched_text, similarity = best_match\n",
        "    print(f\"✅ أنت الآن عند ({surah_number}:{ayah_number}) -(صفحة {page_number}) - {matched_text} (دقة {similarity*100:.2f}%)\")\n",
        "\n",
        "    next_ayah = next((\n",
        "        ((sura, aya, page), text)\n",
        "        for (sura, aya, page), text in quran_texts\n",
        "        if sura == surah_number and aya == ayah_number + 1\n",
        "    ), None)\n",
        "\n",
        "    if next_ayah:\n",
        "        (next_sura, next_ayah_num, next_page), next_text = next_ayah\n",
        "        print(f\"➡️ الآية المتوقعة بعد كده: ({next_sura}:{next_ayah_num}) - {next_text} (صفحة {next_page})\")\n",
        "    else:\n",
        "        # البحث عن أول آية في السورة التالية\n",
        "      next_surah_ayah = next((\n",
        "          ((sura, aya, page), text)\n",
        "          for (sura, aya, page), text in quran_texts\n",
        "          if sura == surah_number + 1 and aya == 1\n",
        "      ), None)\n",
        "\n",
        "      if next_surah_ayah:\n",
        "          (next_sura, next_ayah_num, next_page), next_text = next_surah_ayah\n",
        "          print(f\"⏭️ تم الانتقال إلى السورة التالية ({next_sura}:1) - {next_text} (صفحة {next_page})\")\n",
        "      else:\n",
        "          print(\"⏸️ لا توجد آيات تالية، انتهى المصحف.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "vI7GJQQ6uDfz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51d90a67-75a9-486e-d383-a29afbec9bcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2025-04-08 16:37:45 nemo_logging:393] Using RNNT Loss : warprnnt_numba\n",
            "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Transcribing: 100%|██████████| 1/1 [00:22<00:00, 22.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ أنت الآن عند (1:1) -(صفحة 1) - بسم الله الرحمن الرحيم (دقة 100.00%)\n",
            "➡️ الآية المتوقعة بعد كده: (1:2) - الحمد لله رب العالمين (صفحة 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nemo.collections.asr as nemo_asr\n",
        "import numpy as np\n",
        "from difflib import SequenceMatcher\n",
        "import librosa\n",
        "\n",
        "\n",
        "audio_file_path = '/content/001007.wav'\n",
        "\n",
        "# Get the actual duration of the audio file\n",
        "audio_duration, sample_rate = librosa.get_duration(filename=audio_file_path), librosa.get_samplerate(audio_file_path)\n",
        "print(f\"Audio duration: {audio_duration:.2f}s | Sample rate: {sample_rate}Hz\")\n",
        "# asr_model = nemo_asr.models.EncDecHybridRNNTCTCBPEModel.from_pretrained(model_name=\"nvidia/stt_ar_fastconformer_hybrid_large_pcd_v1.0\")\n",
        "\n",
        "\n",
        "# Transcribe the audio\n",
        "# transcription = asr_model.transcribe([audio_file_path], channel_selector=0, return_hypotheses=True,timestamps= True)[0]\n",
        "# transcription\n"
      ],
      "metadata": {
        "id": "XvXAkYEshu_b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "895ae928-26ae-4bf2-dc45-dc9d4724c25c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Audio duration: 13.19s | Sample rate: 44100Hz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import nemo_asr and instantiate asr_model as above\n",
        "import nemo.collections.asr as nemo_asr\n",
        "asr_model = nemo_asr.models.ASRModel.from_pretrained(\"nvidia/stt_ar_fastconformer_hybrid_large_pcd_v1.0\")\n",
        "\n",
        "# specify flag `timestamps=True`\n",
        "hypotheses = asr_model.transcribe([audio_file_path], channel_selector=0, timestamps= True)\n",
        "\n",
        "\n",
        "\n",
        "# segment level timestamps (if model supports Punctuation and Capitalization, segment level timestamps are displayed based on punctuation otherwise complete transcription is considered as a single segment)"
      ],
      "metadata": {
        "id": "e9ulmyRPSOje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# by default, timestamps are enabled for char, word and segment level\n",
        "word_timestamps = hypotheses[0].timestamp['word'] # word level timestamps for first sample\n",
        "segment_timestamps = hypotheses[0].timestamp['segment'] # segment level timestamps\n",
        "char_timestamps = hypotheses[0].timestamp['char'] # char level timestamps\n",
        "for word in word_timestamps:\n",
        "    print(f\"start: {word['start']}s - end: {word['end']}s word: {word['word']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKWb59EqS80l",
        "outputId": "de2dfcaa-db2a-48d8-d6f6-e0f8d5a9cf9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start: 0.16s - end: 0.8s word: صراط\n",
            "start: 1.04s - end: 1.12s word: الذين\n",
            "start: 1.68s - end: 2.4s word: أنعمت\n",
            "start: 2.8000000000000003s - end: 3.36s word: عليهم\n",
            "start: 3.68s - end: 3.7600000000000002s word: غير\n",
            "start: 4.4s - end: 5.28s word: المغضوب\n",
            "start: 5.6000000000000005s - end: 6.08s word: عليهم\n",
            "start: 6.4s - end: 6.48s word: ولا\n",
            "start: 6.96s - end: 10.32s word: الضالين\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Get time stride from model config\n",
        "time_stride = asr_model.cfg.preprocessor.window_size  # e.g., 0.025s (25ms)\n",
        "print(f\"Time stride: {time_stride}s\")\n",
        "\n",
        "# Extract raw timestamps (frame indices) and total length\n",
        "raw_timestamps = transcription.timestamp.cpu().tolist()  # Frame indices: [2, 5, 9, ...]\n",
        "total_frames = transcription.length.item()  # Total frames: 165\n",
        "print(f\"Raw timestamps (frames): {raw_timestamps[:5]}... (total: {len(raw_timestamps)})\")\n",
        "print(f\"Total frames: {total_frames}\")\n",
        "\n",
        "# Convert raw timestamps to seconds (before scaling)\n",
        "timestamps = [t * time_stride for t in raw_timestamps]\n",
        "print(f\"Max timestamp (before scaling): {timestamps[-1]:.2f}s\")\n",
        "\n",
        "# Scale timestamps to match the full audio duration\n",
        "if timestamps[-1] < audio_duration:\n",
        "    print(f\"⚠️ Model timestamps ({timestamps[-1]:.2f}s) don’t span full audio ({audio_duration:.2f}s). Scaling based on total frames.\")\n",
        "    scale_factor = audio_duration / (total_frames * time_stride)  # Scale based on total frames (165 * 0.025 = 4.125s)\n",
        "    timestamps = [t * time_stride * scale_factor for t in raw_timestamps]\n",
        "    print(f\"Scaled max timestamp: {timestamps[-1]:.2f}s\")\n",
        "\n",
        "# Get the transcription text and split into words\n",
        "text = transcription.text\n",
        "words = text.split()\n",
        "print(f\"Transcription: {text}\")\n",
        "\n",
        "# Debugging: Check lengths\n",
        "print(f\"Timestamps: {len(timestamps)} | Words: {len(words)}\")\n",
        "\n",
        "# Map timestamps to words\n",
        "word_timestamps = []\n",
        "if len(timestamps) > len(words):\n",
        "    print(\"⚠️ More timestamps than words. Grouping timestamps to approximate word boundaries.\")\n",
        "    timestamps_per_word = len(timestamps) // len(words)\n",
        "    for i, word in enumerate(words):\n",
        "        start_idx = i * timestamps_per_word\n",
        "        end_idx = (i + 1) * timestamps_per_word if (i + 1) * timestamps_per_word < len(timestamps) else len(timestamps) - 1\n",
        "        start_time = timestamps[start_idx]\n",
        "        end_time = timestamps[end_idx]\n",
        "        word_timestamps.append({\"word\": word, \"start\": start_time, \"end\": end_time})\n",
        "else:\n",
        "    print(\"⚠️ Fewer or equal timestamps to words. Distributing evenly over full duration.\")\n",
        "    step = audio_duration / len(words)\n",
        "    word_timestamps = [{\"word\": w, \"start\": i * step, \"end\": (i + 1) * step} for i, w in enumerate(words)]\n",
        "\n",
        "# Print the timestamps\n",
        "print(\"🕒 تفاصيل الكلمات:\")\n",
        "for entry in word_timestamps:\n",
        "    duration = entry[\"end\"] - entry[\"start\"]\n",
        "    print(f\"📌 كلمة: {entry['word']} | ⏳ Start: {entry['start']:.2f}s | 🏁 End: {entry['end']:.2f}s | ⏱ Duration: {duration:.2f}s\")"
      ],
      "metadata": {
        "id": "ng1hGt_9hfAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stream"
      ],
      "metadata": {
        "id": "6rM2JGOFvTl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wave\n",
        "import queue\n",
        "import threading\n",
        "import time\n",
        "\n",
        "# تحميل ملف الصوت\n",
        "audio_file_path = \"010010.wav\"\n",
        "wf = wave.open(audio_file_path, 'rb')\n",
        "\n",
        "audio_queue = queue.Queue()\n",
        "\n",
        "def fake_audio_stream():\n",
        "    while True:\n",
        "        data = wf.readframes(1024)\n",
        "        if not data:\n",
        "            break\n",
        "        audio_queue.put(data)\n",
        "        time.sleep(1024 / 16000)\n",
        "\n",
        "def live_transcribe():\n",
        "    print(\"🎙️ استمع الآن... (محاكاة تسجيل لايف)\")\n",
        "\n",
        "    while True:\n",
        "        frames = []\n",
        "        while not audio_queue.empty():\n",
        "            frames.append(audio_queue.get())\n",
        "\n",
        "\n",
        "        if frames:\n",
        "            audio_data = b''.join(frames)\n",
        "\n",
        "            transcription = asr_model.transcribe([audio_data], channel_selector=0)[0]\n",
        "            print(f\"📝 النص: {transcription}\")\n",
        "\n",
        "            best_match = find_best_match(transcription, quran_texts)\n",
        "            if best_match:\n",
        "                (surah_number, ayah_number,page_number), matched_text, similarity = best_match\n",
        "                print(f\"✅ أنت الآن عند ({surah_number}:{ayah_number}) صفحة {page_number} - {matched_text} (دقة {similarity*100:.2f}%)\")\n",
        "\n",
        "                next_ayah = next(((sura, aya,page), text) for (sura, aya,page), text in quran_texts if sura == surah_number and aya == ayah_number + 1)\n",
        "                print(f\"➡️ الآية المتوقعة بعد كده: {next_ayah[1]}\")\n",
        "            else:\n",
        "                print(\"❌ لم أتعرف على الآية\")\n",
        "\n",
        "# تشغيل المحاكاة في Thread منفصل\n",
        "threading.Thread(target=fake_audio_stream, daemon=True).start()\n",
        "threading.Thread(target=live_transcribe, daemon=True).start()\n"
      ],
      "metadata": {
        "id": "rdP_klB7vTQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://wtq-recitations.s3.us-east-2.amazonaws.com/Word_by-Word/2/002007001.mp3\n",
        "#https://wtq-recitations.s3.us-east-2.amazonaws.com/Word_by-Word/[surah number]/[surah number][ayah number][word number].mp3\n",
        "import os\n",
        "import requests\n",
        "# make a code that will download all the audio files from the above link\n",
        "def download_word_by_word(surah_number, ayah_number, word_number):\n",
        "    file_name = f'{surah_number:03}{ayah_number:03}{word_number:03}.mp3'\n",
        "    url =f'https://wtq-recitations.s3.us-east-2.amazonaws.com/Word_by-Word/{surah_number}/{surah_number:03}{ayah_number:03}{word_number:03}.mp3'\n",
        "    base_folder = 'word_by_word'\n",
        "    surah_folder = f'tajweed/surah_{surah_number}'\n",
        "\n",
        "    if not os.path.exists(base_folder):\n",
        "        os.makedirs(base_folder)\n",
        "\n",
        "    if not os.path.exists(os.path.join(base_folder, surah_folder)):\n",
        "        os.makedirs(os.path.join(base_folder, surah_folder))\n",
        "\n",
        "    file_path = os.path.join(base_folder, surah_folder, file_name)\n",
        "\n",
        "    if os.path.exists(file_path):\n",
        "        print(f'{file_name} already exists')\n",
        "        return\n",
        "\n",
        "    response = requests.get(url)\n",
        "    print(str(response.status_code) + \" Connection Okay: \" + url)\n",
        "    if response.status_code == 200:\n",
        "        with open(file_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "        print(f'Downloaded and saved to {file_path}')\n",
        "    else:\n",
        "        print(f'Failed to download {file_name}')"
      ],
      "metadata": {
        "id": "WH7re3qBm6UQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Production"
      ],
      "metadata": {
        "id": "FaiGxf8Tjo56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from difflib import SequenceMatcher\n",
        "import nemo.collections.asr as nemo_asr\n",
        "\n",
        "class QuranASR:\n",
        "    def __init__(self, model_name=\"nvidia/stt_ar_fastconformer_hybrid_large_pcd_v1.0\", quran_json_path=\"/content/quran.json\"):\n",
        "        self.asr_model = nemo_asr.models.EncDecHybridRNNTCTCBPEModel.from_pretrained(model_name=model_name)\n",
        "        self.quran_texts = self._load_quran(quran_json_path)\n",
        "\n",
        "    def _load_quran(self, json_path):\n",
        "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            quran_data = json.load(f)\n",
        "        return [((s[\"surah_number\"], s[\"verse_number\"], s[\"page\"]), s[\"text\"]) for s in quran_data]\n",
        "\n",
        "    def _word_similarity(self, word1, word2):\n",
        "        return SequenceMatcher(None, word1, word2).ratio()\n",
        "\n",
        "    def _sentence_similarity(self, partial_text, full_text):\n",
        "        partial_words = list(set(partial_text.split()))  # إزالة التكرار\n",
        "        full_words = full_text.split()\n",
        "\n",
        "        match_count, total_weight = 0, 0\n",
        "        for p_word in partial_words:\n",
        "            best_match = max((self._word_similarity(p_word, f_word) for f_word in full_words), default=0)\n",
        "            if best_match > 0.7:  # رفع العتبة لتحسين الدقة\n",
        "                match_count += 1\n",
        "                total_weight += best_match\n",
        "\n",
        "        return total_weight / len(partial_words) if partial_words else 0\n",
        "\n",
        "    def _find_best_match(self, spoken_text, page_number):\n",
        "        best_match, best_ratio = None, 0\n",
        "        filtered_quran_texts = [entry for entry in self.quran_texts if entry[0][2] == page_number]\n",
        "\n",
        "        for (surah_ayah_page, text) in filtered_quran_texts:\n",
        "            similarity = self._sentence_similarity(spoken_text, text)\n",
        "            if similarity > best_ratio:\n",
        "                best_ratio = similarity\n",
        "                best_match = (surah_ayah_page, text, similarity)\n",
        "\n",
        "        return best_match if best_ratio > 0.5 else None  # رفع العتبة إلى 50% للتحسين\n",
        "\n",
        "    def process_audio(self, audio_file_path):\n",
        "        transcriptions = self.asr_model.transcribe([audio_file_path], channel_selector=0)[0]\n",
        "        return transcriptions.text.strip()\n",
        "\n",
        "    def get_next_quran_word(self, audio_file_path, page_number):\n",
        "        spoken_text = self.process_audio(audio_file_path)\n",
        "        best_match = self._find_best_match(spoken_text, page_number)\n",
        "\n",
        "        if not best_match:\n",
        "            return {\"spoken_text\": spoken_text, \"error\": \"No matching verse found\"}\n",
        "\n",
        "        (surah_number, ayah_number, page_number), matched_text, similarity = best_match\n",
        "        words = matched_text.split()\n",
        "        spoken_words = list(set(spoken_text.split()))  # إزالة الكلمات المتكررة\n",
        "\n",
        "        next_word = None\n",
        "        for i in range(len(spoken_words) - 1, -1, -1):\n",
        "            if spoken_words[i] in words:\n",
        "                index = words.index(spoken_words[i])\n",
        "                if index + 1 < len(words):\n",
        "                    next_word = words[index + 1]\n",
        "                break\n",
        "\n",
        "        if not next_word:\n",
        "            next_ayah = next(\n",
        "                (((sura, aya, page), text) for (sura, aya, page), text in self.quran_texts if sura == surah_number and aya == ayah_number + 1), None\n",
        "            )\n",
        "            next_word = next_ayah[1].split()[0] if next_ayah else None\n",
        "\n",
        "        return {\n",
        "            \"spoken_text\": spoken_text,\n",
        "            \"current\": {\n",
        "                \"surah\": surah_number,\n",
        "                \"ayah\": ayah_number,\n",
        "                \"page\": page_number,\n",
        "                \"text\": matched_text,\n",
        "                \"accuracy\": round(similarity * 100, 2)\n",
        "            },\n",
        "            \"next_word\": next_word\n",
        "        }\n"
      ],
      "metadata": {
        "id": "0ybU7UiqjooG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quran_asr = QuranASR()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wJEnq5AXjoku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "for surah in os.listdir('/content/word_by_word/tajweed'):\n",
        "    ayahs = os.path.join('/content/word_by_word/tajweed', surah)\n",
        "    print(ayahs[-1])\n",
        "    for ayah in os.listdir(ayahs):\n",
        "        audio_file_path = os.path.join(ayahs, ayah)\n",
        "        result = quran_asr.get_next_quran_word(audio_file_path,int(ayahs[-1]))\n",
        "        results.append(result)\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "jZpouTv0joii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(json.dumps(results, ensure_ascii=False, indent=4))\n",
        "with open('output.json', 'w', encoding='utf-8') as f:\n",
        "       f.write(json.dumps(results, ensure_ascii=False, indent=4))"
      ],
      "metadata": {
        "id": "upn7ePtVotsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open('ayahs.json', 'r', encoding='utf-8') as ayahs:\n",
        "    ayahs = json.load(ayahs)\n",
        "    for ayah in ayahs:\n",
        "        break\n",
        "        for index , word in enumerate(ayah['words']):\n",
        "            download_word_by_word(ayah['surah_number'],ayah['verse_number'], index+1)\n"
      ],
      "metadata": {
        "id": "uLFiogmSjogR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyArabic"
      ],
      "metadata": {
        "id": "vNPHdGZOxP6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from difflib import SequenceMatcher\n",
        "\n",
        "def word_similarity(word1, word2):\n",
        "    return SequenceMatcher(None, word1, word2).ratio()\n",
        "\n",
        "def sentence_similarity(partial_text, full_text):\n",
        "    partial_words = partial_text.split()\n",
        "    full_words = full_text.split()\n",
        "\n",
        "    match_count = 0\n",
        "    total_weight = 0\n",
        "\n",
        "    for p_word in partial_words:\n",
        "        best_match = max((word_similarity(p_word, f_word) for f_word in full_words), default=0)\n",
        "        if best_match > 0.5:\n",
        "            match_count += 1\n",
        "            total_weight += best_match\n",
        "\n",
        "    return total_weight / len(partial_words) if partial_words else 0\n",
        "\n",
        "def _find_best_match(partial_text, page_number):\n",
        "    best_match = None\n",
        "    best_ratio = 0\n",
        "\n",
        "    filtered_quran_texts = [entry for entry in quran_texts if entry[0][2] == page_number]\n",
        "    print(filtered_quran_texts)\n",
        "    for (surah_ayah_page, text) in filtered_quran_texts:\n",
        "        similarity = sentence_similarity(partial_text, text)\n",
        "        if similarity > best_ratio:\n",
        "            best_ratio = similarity\n",
        "            best_match = (surah_ayah_page, text, similarity)\n",
        "\n",
        "    return best_match if best_ratio > 0.3 else None\n",
        "\n",
        "# ✅ تجربة البحث بجزء بسيط من الآية\n",
        "print(_find_best_match('بسم', 1))\n",
        "print(_find_best_match('الرحمن الرحيم', 1))\n",
        "print(_find_best_match('اهدنا الصراط المستقيم', 1))\n"
      ],
      "metadata": {
        "id": "EjJj9Qbvr80i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}